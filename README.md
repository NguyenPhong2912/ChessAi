# Chess AI with Advanced Deep Learning

D·ª± √°n n√†y l√† m·ªôt AI c·ªù vua th√¥ng minh s·ª≠ d·ª•ng k·∫øt h·ª£p nhi·ªÅu k·ªπ thu·∫≠t h·ªçc s√¢u ti√™n ti·∫øn:
- MCTS (Monte Carlo Tree Search)
- Q-Learning v·ªõi Deep Neural Network
- ADMCDL (Adaptive Meta-Controller Deep Learning)

## üéÆ T√≠nh nƒÉng ch√≠nh

1. **Giao di·ªán tr·ª±c quan**
   - B√†n c·ªù ƒë·∫πp m·∫Øt v·ªõi hi·ªáu ·ª©ng animation
   - Hi·ªÉn th·ªã g·ª£i √Ω n∆∞·ªõc ƒëi h·ª£p l·ªá
   - Panel th√¥ng tin hi·ªÉn th·ªã l∆∞·ª£t ƒëi v√† tr·∫°ng th√°i game
   - H·ªó tr·ª£ k√©o th·∫£ qu√¢n c·ªù v·ªõi hi·ªáu ·ª©ng m∆∞·ª£t m√†

2. **AI th√¥ng minh**
   - S·ª≠ d·ª•ng MCTS khi epsilon < 0.1 (200 l·∫ßn m√¥ ph·ªèng)
   - Q-Learning v·ªõi replay memory 50,000 v√† batch size 128
   - DNN v·ªõi ki·∫øn tr√∫c 1024 -> 512 -> 256 -> output_size
   - ADMCDL v·ªõi 4 options cho c√°c chi·∫øn l∆∞·ª£c kh√°c nhau

3. **Lu·∫≠t ch∆°i ƒë·∫ßy ƒë·ªß**
   - H·ªó tr·ª£ t·∫•t c·∫£ lu·∫≠t c·ªù vua chu·∫©n
   - Phong t·ªët v·ªõi menu l·ª±a ch·ªçn qu√¢n c·ªù
   - Chi·∫øu t∆∞·ªõng v√† c√°c ƒëi·ªÅu ki·ªán h√≤a
   - Lu·∫≠t ƒë·∫∑c bi·ªát: khi ch·ªâ c√≤n vua, ng∆∞·ªùi ƒÉn qu√¢n cu·ªëi th·∫Øng

## üìÅ C·∫•u tr√∫c d·ª± √°n v√† chi ti·∫øt module

### 1. Game Logic (`scr/game/`)
- **ChessEngine.py** (11KB)
  - X·ª≠ l√Ω logic c·ªù vua ch√≠nh
  - ƒê√°nh gi√° v·ªã tr√≠ v√† t√≠nh to√°n n∆∞·ªõc ƒëi
  - T√≠ch h·ª£p v·ªõi c√°c agent AI
- **ChessRules.py** (17KB)
  - Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa n∆∞·ªõc ƒëi
  - X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát (phong t·ªët, chi·∫øu)
  - Qu·∫£n l√Ω lu·∫≠t ch∆°i
- **ChessBoard.py** (6.7KB)
  - Qu·∫£n l√Ω tr·∫°ng th√°i b√†n c·ªù
  - X·ª≠ l√Ω di chuy·ªÉn qu√¢n c·ªù
  - L∆∞u tr·ªØ l·ªãch s·ª≠ n∆∞·ªõc ƒëi

### 2. AI Agents (`scr/agents/`)
- **q_learning_agent.py** (6.8KB)
  - Agent h·ªçc Q-Learning ch√≠nh
  - T√≠ch h·ª£p v·ªõi replay memory
  - X·ª≠ l√Ω epsilon-greedy exploration
- **mcts.py** (2.3KB)
  - Monte Carlo Tree Search
  - 200 simulations m·ªói n∆∞·ªõc ƒëi
  - UCB1 selection policy
- **experience_replay.py** (1.4KB)
  - B·ªô nh·ªõ replay v·ªõi capacity 50,000
  - Batch sampling cho training
  - Prioritized experience replay

### 3. Neural Networks (`scr/models/`)
- **advanced_q_network.py** (12KB)
  - M·∫°ng neural v·ªõi ADMCDL
  - 4 options cho c√°c chi·∫øn l∆∞·ª£c
  - Batch normalization v√† dropout
- **q_network.py** (1.3KB)
  - M·∫°ng Q c∆° b·∫£n
  - Ki·∫øn tr√∫c 1024 -> 512 -> 256
- **ModelTrainer.py** (4.7KB)
  - Hu·∫•n luy·ªán model v·ªõi early stopping
  - C·∫≠p nh·∫≠t target network
  - L∆∞u v√† load model

### 4. Training (`scr/training/`)
- **trainer.py** (4.4KB)
  - Hu·∫•n luy·ªán AI v·ªõi reward shaping
  - X·ª≠ l√Ω episode v√† batch training
  - Theo d√µi metrics
- **model_evaluator.py** (3.6KB)
  - ƒê√°nh gi√° model qua win rate
  - So s√°nh v·ªõi c√°c phi√™n b·∫£n
  - Logging k·∫øt qu·∫£
- **early_stopping.py** (2.0KB)
  - D·ª´ng s·ªõm khi kh√¥ng c·∫£i thi·ªán
  - Theo d√µi validation loss
  - L∆∞u model t·ªët nh·∫•t

### 5. ADMCDL (`scr/Adaptive Meta-Controller Deep Q-learning/`)
- **train.py** (8.6KB)
  - Hu·∫•n luy·ªán ADMCDL
  - Meta-learning controller
  - Option training
- **meta_controller.py** (4.1KB)
  - ƒêi·ªÅu khi·ªÉn meta-learning
  - Option selection
  - Policy adaptation
- **dqn_agent.py** (7.8KB)
  - Deep Q-Network agent
  - Experience replay
  - Target network
- **chess_env.py** (3.4KB)
  - M√¥i tr∆∞·ªùng c·ªù vua
  - State representation
  - Reward calculation

### 6. H·ªó tr·ª£
- **utils/**: C√¥ng c·ª• h·ªó tr·ª£
- **visualization/**: Hi·ªÉn th·ªã v√† animation
- **data/**: L∆∞u tr·ªØ d·ªØ li·ªáu hu·∫•n luy·ªán
- **train_model.py**: Script hu·∫•n luy·ªán ch√≠nh

## üîß Y√™u c·∫ßu h·ªá th·ªëng v√† th∆∞ vi·ªán

### Python 3.9+
- **PyTorch**: Deep learning framework
- **Pygame**: Giao di·ªán ƒë·ªì h·ªça
- **python-chess**: X·ª≠ l√Ω logic c·ªù vua
- **numpy**: X·ª≠ l√Ω m·∫£ng v√† t√≠nh to√°n
- **pandas**: X·ª≠ l√Ω d·ªØ li·ªáu v√† logging
- **matplotlib**: V·∫Ω ƒë·ªì th·ªã v√† visualization

### C√†i ƒë·∫∑t
```bash
# Clone repository
git clone [url]

# T·∫°o m√¥i tr∆∞·ªùng ·∫£o
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# C√†i ƒë·∫∑t dependencies
pip install -r requirements.txt
```

## üéØ C√°ch ch∆°i

### 1. Ch∆°i v·ªõi AI
```bash
python main.py
```

**ƒêi·ªÅu khi·ªÉn:**
- Click v√† k√©o qu√¢n c·ªù ƒë·ªÉ di chuy·ªÉn
- C√°c n∆∞·ªõc ƒëi h·ª£p l·ªá s·∫Ω ƒë∆∞·ª£c highlight m√†u xanh
- Th·∫£ qu√¢n c·ªù v√†o √¥ h·ª£p l·ªá ƒë·ªÉ th·ª±c hi·ªán n∆∞·ªõc ƒëi
- Khi t·ªët ƒë·∫øn cu·ªëi b√†n c·ªù, menu phong s·∫Ω hi·ªán ra
- Ch·ªçn qu√¢n c·ªù mu·ªën phong (h·∫≠u, xe, t∆∞·ª£ng, m√£)

**Lu·∫≠t ch∆°i:**
- Chi·∫øu h·∫øt: ng∆∞·ªùi chi·∫øu th·∫Øng
- H√≤a do b·∫ø t·∫Øc ho·∫∑c kh√¥ng c√≤n n∆∞·ªõc ƒëi
- Khi ch·ªâ c√≤n vua: ng∆∞·ªùi ƒÉn qu√¢n cu·ªëi c√πng th·∫Øng

### 2. Hu·∫•n luy·ªán AI

#### Hu·∫•n luy·ªán v·ªõi UI
```bash
python notebook/train_chess_ai_with_ui.py
```
- Hi·ªÉn th·ªã b√†n c·ªù v√† th√¥ng tin hu·∫•n luy·ªán
- Theo d√µi reward, win rate, epsilon
- L∆∞u model t·ªët nh·∫•t t·ª± ƒë·ªông

#### Hu·∫•n luy·ªán kh√¥ng c√≥ UI
```bash
python scr/train_model.py
```
- Hu·∫•n luy·ªán nhanh h∆°n
- L∆∞u log chi ti·∫øt
- ƒê√°nh gi√° model sau m·ªói episode

**Tham s·ªë hu·∫•n luy·ªán:**
- S·ªë episode: 500
- S·ªë n∆∞·ªõc ƒëi t·ªëi ƒëa: 200
- Th·ªùi gian t·ªëi ƒëa: 600s
- Early stopping: 50 episode kh√¥ng c·∫£i thi·ªán

## üìä ƒê√°nh gi√° hi·ªáu su·∫•t

1. **Win Rate**
   - ƒê√°nh gi√° qua 100 v√°n ƒë·∫•u
   - So s√°nh v·ªõi c√°c phi√™n b·∫£n tr∆∞·ªõc

2. **Reward**
   - T·ªïng reward m·ªói episode
   - Reward trung b√¨nh
   - ƒê·ªô ·ªïn ƒë·ªãnh

3. **Th·ªùi gian**
   - Th·ªùi gian hu·∫•n luy·ªán
   - Th·ªùi gian suy lu·∫≠n m·ªói n∆∞·ªõc ƒëi

## üìù Li√™n h·ªá (contacts)

N·∫øu c√≥ g√≥p √Ω ho·∫∑c c·∫ßn h·ªó tr·ª£, vui l√≤ng li√™n h·ªá:
- Personal Email: ntphong1231@gmail.com
- Working Email: Phong.2474802010304@vanlanguni.vn
## üìú License

VLUVN - VANLANG UNIVERSITY VIETNAM - FACUTLY OF INFORMATION TECHNOLOGY VLU.  
AUTHOR : 
NGUYEN THANH PHONG, 
ASSOCIATES: 
- TIEN TRI NGUYEN
- NGUYEN HA THU
INSTRUCTOR:
NGUYEN THE AN,
